{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15277d25-374f-4f03-8dd6-0fbeddea7d1b",
   "metadata": {},
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5dd7a8-e83a-4964-9c40-1e74cca7726d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chains import ConversationChain, HypotheticalDocumentEmbedder, LLMChain\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.chat_models import ChatMlflow\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import BaseTool, Tool\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from domino_data.vectordb import DominoPineconeConfiguration\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "import os\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08545e-ea3e-4163-aaf6-f70fd791ec73",
   "metadata": {},
   "source": [
    "### Load the embedding model and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a738335-8aa1-4804-aa49-abe386d83362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd6c38b1-47e2-451b-9e39-ffde7db7cfab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domino/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_name = \"BAAI/bge-small-en\"\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/mnt/data/{}/model_cache/'.format(os.environ['DOMINO_PROJECT_NAME'])\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=embedding_model_name,\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6d845c-bb8c-4ec6-b3de-61c0f8768bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Pinecone index\n",
    "datasource_name = \"pinecone-domino-support\"\n",
    "conf = DominoPineconeConfiguration(datasource=datasource_name)\n",
    "api_key = os.environ.get(\"DOMINO_VECTOR_DB_METADATA\", datasource_name)\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=api_key,\n",
    "    environment=\"domino\",\n",
    "    openapi_config=conf,\n",
    ")\n",
    "\n",
    "index = pinecone.Index(\"domino-support\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c2b3e-ee1c-48be-817f-e736e4fe2ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatMlflow(\n",
    "    target_uri=os.environ[\"DOMINO_MLFLOW_DEPLOYMENTS\"],\n",
    "    endpoint=\"Chat-gpt35turbo-se\",\n",
    ")\n",
    "\n",
    "conversation_openai = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=ConversationSummaryMemory(llm=chat),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# conversation_anthropic = ConversationChain(\n",
    "#         llm=ChatAnthropic(model='claude-2.1'),\n",
    "#         memory=ConversationSummaryMemory(llm=ChatAnthropic(model='claude-2.1')),\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"How can I help you today?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e742ca-d6f4-4a20-a176-16f24e6f0a3a",
   "metadata": {},
   "source": [
    "### Set up HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "HyDe promt templates improve the model response by rephrasing the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061e75f-b198-43d9-916b-32cf0231edb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup HyDE\n",
    "\n",
    "hyde_prompt_template = \"\"\"You are a virtual assistant for Domino and your task is to answer questions related to Domino which includes general information about Domino\n",
    "\"Please answer the user's question below \\n \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "hyde_prompt = PromptTemplate(input_variables=[\"question\"], template=hyde_prompt_template)\n",
    "hyde_llm_chain = LLMChain(llm=chat, prompt=hyde_prompt)\n",
    "\n",
    "hyde_embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=hyde_llm_chain, base_embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f122352-21a0-4409-a135-04d2bfa76a3b",
   "metadata": {},
   "source": [
    "### Improving the results\n",
    "\n",
    "#### Re-Ranking with ColBERT\n",
    "\n",
    "ColBERT is a model that ranks the vectors returned from the Vector DB to improve the relevance of the results.\n",
    "\n",
    "#### Add a template for the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f8710-fc11-4444-9f8c-235141299e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get relevant docs through vector DB\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.2\n",
    "\n",
    "# Number of texts to match (may be less if no suitable match)\n",
    "NUM_TEXT_MATCHES = 5\n",
    "\n",
    "# Number of texts to return from reranking\n",
    "NUM_RERANKING_MATCHES = 3\n",
    "\n",
    "# Create prompt\n",
    "template = \"\"\" You are a virtual assistant for Domino and your task is to answer questions related to Domino which includes general information about Domino.\n",
    "\n",
    "                Respond in the style of a polite helpful assistant and do not allude that you have looked up the context.\n",
    "\n",
    "                Do not hallucinate. If you don't find an answer, you can point user to the official website here: https://docs.dominodatalab.com/ . \n",
    "\n",
    "                In your response, include the following url links at the end of your response {url_links} and any other relevant URL links that you refered.\n",
    "\n",
    "                Also, at the end of your response, ask if your response was helpful\". \n",
    "\n",
    "                Here is some relevant context: {context}\"\"\"\n",
    "\n",
    "# Load the reranking model\n",
    "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Get relevant docs through vector DB\n",
    "def get_relevant_docs(user_input, num_matches=NUM_TEXT_MATCHES, use_hyde=True):\n",
    "   \n",
    "    if use_hyde:\n",
    "        embedded_query = hyde_embeddings.embed_query(user_input)\n",
    "    else:\n",
    "        embedded_query = embeddings.embed_query(user_input)\n",
    "    \n",
    "    relevant_docs = index.query(\n",
    "        namespace='domino-help',\n",
    "        vector=embedded_query,\n",
    "        top_k=num_matches,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    matches = relevant_docs[\"matches\"]\n",
    "    filtered_matches = [match for match in matches if match['score'] >= SIMILARITY_THRESHOLD]\n",
    "    relevant_docs[\"matches\"] = filtered_matches\n",
    "\n",
    "    return relevant_docs\n",
    "\n",
    " \n",
    "def build_system_prompt(user_input, rerank=True, use_hyde=True):\n",
    "    \n",
    "    try:\n",
    "        relevant_docs = get_relevant_docs(user_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get relevant documents: {e}\")\n",
    "        return \"\", \"Failed to get relevant documents\"\n",
    "    \n",
    "    actual_num_matches = len(relevant_docs[\"matches\"])\n",
    "    if actual_num_matches == 0:\n",
    "        print(\"No matches found in relevant documents.\")\n",
    "        return \"\", \"No matches found in relevant documents\"\n",
    "    \n",
    "    urls = set([relevant_docs[\"matches\"][i][\"metadata\"][\"source\"] for i in range(actual_num_matches)])\n",
    "    contexts = [relevant_docs[\"matches\"][i][\"metadata\"][\"text\"] for i in range(actual_num_matches)]\n",
    "    \n",
    "    if rerank and actual_num_matches >= NUM_RERANKING_MATCHES:\n",
    "        print(actual_num_matches)\n",
    "        try:\n",
    "            docs = colbert.rerank(query=user_input, documents=contexts, k=NUM_RERANKING_MATCHES)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to rerank documents: {e}\")\n",
    "            return \"\", \"Failed to rerank documents\"\n",
    "        \n",
    "        try:\n",
    "            result_indices = [docs[i][\"result_index\"] for i in range(NUM_RERANKING_MATCHES)]\n",
    "        except (IndexError, KeyError) as e:\n",
    "                print(f\"Invalid result indices: {e}\")\n",
    "                return \"\", \"Invalid result indices\"\n",
    "        try:    \n",
    "            contexts = [contexts[index] for index in result_indices]\n",
    "            urls = [list(urls)[index] for index in result_indices]\n",
    "        except IndexError as e:\n",
    "            print(f\"Indexing error: {e}\")\n",
    "            return \"\", \"Indexing error\"\n",
    "    \n",
    "    # Create prompt\n",
    "    template = \"\"\" You are a virtual assistant for Domino Data Lab and your task is to answer questions related to Domino which includes general information about Domino.\n",
    "\n",
    "                Respond in the style of a polite helpful assistant and do not allude that you have looked up the context.\n",
    "\n",
    "                Do not hallucinate. If you don't find an answer, you can point user to the official website here: https://docs.dominodatalab.com/. \n",
    "\n",
    "                In your response, include the following url links at the end of your response {url_links} and any other relevant URL links that you refered.\n",
    "\n",
    "                Also, at the end of your response, ask if your response was helpful\". \n",
    "\n",
    "                Here is some relevant context: {context}\"\"\"\n",
    " \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"url_links\", \"context\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    system_prompt = prompt_template.format( url_links=urls, context=contexts)\n",
    " \n",
    "    return system_prompt, contexts\n",
    " \n",
    "# Query the Open AI Model\n",
    "def queryAIModel(user_input, llm_name=\"openai\", use_hyde=True):\n",
    "\n",
    "    if not user_input:\n",
    "        return \"Please provide an input\"\n",
    "    \n",
    "    system_prompt = build_system_prompt(user_input)            \n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=system_prompt\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=user_input\n",
    "        ),\n",
    "    ]\n",
    "    if llm_name.lower() == \"openai\":\n",
    "        output = conversation_openai.predict(input=messages)\n",
    "    else:\n",
    "        output = conversation_anthropic.predict(input=messages)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854478ef-aaf8-41a7-b460-9cbdb6ff2ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ask a question ; uncomment to test\n",
    "\n",
    "# What options do I have for adding code packages to my executions?\n",
    "\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = queryAIModel(user_question)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe57134-b2b2-4e7a-9110-356f9a2f7650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Advanced RAG Env",
   "language": "python",
   "name": "test_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
